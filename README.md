# Market Intelligence Pipeline

A production-ready, real-time market intelligence pipeline that discovers and analyzes competitor products through Facebook ads and enriches them with traffic data.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     SSE      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js UI     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  FastAPI Backend â”‚â”€â”€â”€â”€â–¶â”‚  SQLite DB      â”‚
â”‚  (Port 3000)    â”‚              â”‚  (Port 8000)     â”‚     â”‚                 â”‚
â”‚                 â”‚              â”‚                  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Dashboard     â”‚              â”‚ â€¢ REST API       â”‚              â”‚
â”‚ â€¢ Real-time     â”‚              â”‚ â€¢ SSE Streaming  â”‚              â–¼
â”‚   Progress      â”‚              â”‚ â€¢ Metrics        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Results View  â”‚              â”‚ â€¢ Health Checks  â”‚â”€â”€â”€â”€â–¶â”‚ External APIsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â€¢ Apify      â”‚
                                                           â”‚ â€¢ ScraperAPI â”‚
                                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features
- **Real-time Progress Streaming**: Server-Sent Events (SSE) for live pipeline updates
- **Structured Logging**: JSON logs with correlation IDs for request tracing
- **Metrics & Monitoring**: Prometheus-compatible `/metrics` endpoint
- **Retry Logic**: Exponential backoff for external API resilience
- **Full Observability**: Health checks, detailed logging, and error tracking

## ğŸš€ Quick Start

### Using Docker (Recommended)

```bash
# Start the entire stack
docker compose up --build

# Access the application
# UI: http://localhost:3000
# API: http://localhost:8000
# Docs: http://localhost:8000/docs
```

### Manual Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt
cd frontend && npm install

# 2. Set up environment
cp .env.example .env
# Edit .env with your API keys:
# - APIFY_TOKEN
# - SCRAPER_API_KEY

# 3. Start backend
python app_server.py

# 4. Start frontend (new terminal)
cd frontend && npm run dev
```

## ğŸ”§ Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `APIFY_TOKEN` | Apify API token for Facebook ad scraping | Yes |
| `SCRAPER_API_KEY` | ScraperAPI key for traffic data | Yes |
| `CORS_ORIGIN` | Frontend URL for CORS (default: http://localhost:3000) | No |
| `LOG_LEVEL` | Logging level (DEBUG, INFO, WARNING, ERROR) | No |

## ğŸ“Š API Endpoints

### Pipeline Management
- `POST /pipeline/run` - Start analysis for a keyword
- `GET /pipeline/status/{keyword}` - Get pipeline status
- `GET /pipeline/stream/{keyword}` - SSE stream for real-time updates

### Results & Analytics  
- `GET /results/products` - Query discovered products
- `GET /results/traffic` - Get traffic intelligence data
- `GET /dashboard/stats` - Dashboard statistics

### Monitoring
- `GET /health` - Health check
- `GET /metrics` - Prometheus metrics
- `GET /metrics/health/detailed` - Detailed component health

## ğŸ§ª Testing

```bash
# Run backend tests
pytest tests/backend/test_e2e_pipeline.py -v

# Run specific e2e test
pytest tests/backend/test_e2e_pipeline.py::test_full_pipeline_e2e -v

# Run with coverage
pytest --cov=api --cov=app tests/
```

### Test Coverage
- E2E pipeline test with `atlas_test` keyword
- Concurrent pipeline handling
- Error scenarios
- API endpoint verification
- Database integrity checks

## ğŸ“ˆ Monitoring & Observability

### Metrics
Access Prometheus-compatible metrics at `http://localhost:8000/metrics`:
- Request counts and latency
- Pipeline success/failure rates  
- Database statistics
- Memory usage

### Logs
Structured JSON logs are written to `logs/app.log` with:
- Correlation IDs for request tracing
- Log rotation (10MB max, 5 backups)
- Real-time streaming to UI

### Health Checks
- Basic: `GET /health`
- Detailed: `GET /metrics/health/detailed`

## ğŸš¦ Common Operations

### Run Analysis for a Keyword
```bash
# Via API
curl -X POST http://localhost:8000/pipeline/run \
  -H "Content-Type: application/json" \
  -d '{"keyword": "fitness", "max_ads": 20}'

# Via CLI
python run_one_keyword.py --keyword "fitness" --max-ads 20
```

### Monitor Real-time Progress
1. Open dashboard at http://localhost:3000
2. Enter keyword and click "Run Pipeline" 
3. Watch live progress with streaming logs

### Export Results
```bash
# Get products as JSON
curl "http://localhost:8000/results/products?keyword=fitness&format=json" > products.json

# Get traffic data
curl "http://localhost:8000/results/traffic?keyword=fitness" > traffic.json
```

## ğŸ” Troubleshooting

### Pipeline Failures
1. Check logs: `tail -f logs/app.log | jq .`
2. Verify API keys in `.env`
3. Check external API quotas
4. Review correlation ID in logs for failed request

### Connection Issues
- Frontend can't reach backend: Check CORS settings
- SSE disconnects: Check nginx/proxy buffering settings
- Database locked: Ensure single writer with SQLite

### Performance Issues
- Slow Step 1: Apify API latency (normal: 15-20s)
- Slow Step 2: Rate limiting on traffic API
- High memory: Check `/metrics/health/detailed`

## ğŸ“Š Database Schema

### Core Tables
- `keyword` - Search keywords and their status
- `discoveredproduct` - Products found via Facebook ads
- `trafficintelligence` - Website traffic metrics
- `contentanalysis` - AI-powered content classification

### Key Relationships
```
keyword (1) â”€â”€â”€ (N) discoveredproduct (1) â”€â”€â”€ (N) trafficintelligence
                           â”‚
                           â””â”€â”€â”€â”€ (N) contentanalysis
```

## ğŸš€ Scaling Roadmap

### Phase 1: Current (SQLite + Single Instance)
- âœ… Suitable for <100 concurrent users
- âœ… Handles ~1000 keywords/day
- âœ… Simple deployment

### Phase 2: PostgreSQL + Redis
- Multiple API instances
- Redis for caching & queues
- PostgreSQL for concurrency
- ~10,000 keywords/day

### Phase 3: Distributed Processing  
- Celery for task distribution
- S3 for result storage
- Kubernetes deployment
- Unlimited scale

## ğŸ› ï¸ Development

### Code Structure
```
minimal_app/
â”œâ”€â”€ api/                 # FastAPI backend
â”‚   â”œâ”€â”€ routers/        # API endpoints
â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â””â”€â”€ utils/          # Helpers
â”œâ”€â”€ app/                # Core pipeline
â”‚   â”œâ”€â”€ core/          # Scraping logic
â”‚   â””â”€â”€ models/        # Database models  
â”œâ”€â”€ frontend/          # Next.js UI
â””â”€â”€ tests/            # Test suites
```

### Adding New Features
1. Add router in `api/routers/`
2. Implement service in `api/services/`
3. Update models if needed
4. Add tests
5. Update API docs

### Contributing
1. Fork the repository
2. Create feature branch
3. Add tests for new features
4. Ensure all tests pass
5. Submit pull request

## ğŸ“ License

This project is proprietary and confidential.